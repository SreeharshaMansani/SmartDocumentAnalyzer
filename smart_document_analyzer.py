# -*- coding: utf-8 -*-
"""Smart Document Analyzer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1elCO5AXjM8nn9-Ewjxihqu58P7HJtYPr
"""

!pip install transformers torch sentencepiece
!pip install streamlit
!pip install pandas
!pip install pdfplumber python-docx



# Smart Document Analyzer - Colab Version (No Q&A)

from transformers import pipeline
import pdfplumber
import docx
from sklearn.feature_extraction.text import TfidfVectorizer
from google.colab import files
import re

# ===========================
# Load HuggingFace Pipelines
# ===========================
summarizer = pipeline("summarization", model="t5-small")
ner_tagger = pipeline("ner", grouped_entities=True)
sentiment_analyzer = pipeline("sentiment-analysis")

# ===========================
# Helper Functions
# ===========================
def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

def extract_text_from_docx(file_path):
    doc = docx.Document(file_path)
    text = "\n".join([para.text for para in doc.paragraphs])
    return text

def summarize_text(text):
    return summarizer(text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']

def extract_entities(text, min_confidence=0.85):
    entities = ner_tagger(text)
    filtered_entities = [
        e for e in entities
        if e['score'] >= min_confidence and re.match(r"^[A-Za-z][A-Za-z0-9&\-.]+$", e['word'])
    ]
    filtered_entities = merge_technical_tokens(filtered_entities)
    return filtered_entities

def merge_technical_tokens(entities):
    merged_entities = []
    skip_next = False
    for i, e in enumerate(entities):
        if skip_next:
            skip_next = False
            continue
        word = e['word']
        if i + 1 < len(entities):
            next_word = entities[i + 1]['word']
            if re.match(r"[A-Z][a-z]?$", word) and re.match(r"^[A-Z][A-Z]+$", next_word):
                word = word + next_word
                e['word'] = word
                skip_next = True
        merged_entities.append(e)
    return merged_entities

def extract_keywords(text, top_n=10):
    vectorizer = TfidfVectorizer(stop_words='english')
    X = vectorizer.fit_transform([text])
    scores = zip(vectorizer.get_feature_names_out(), X.toarray()[0])
    sorted_words = sorted(scores, key=lambda x: x[1], reverse=True)
    return [word for word, score in sorted_words[:top_n]]

def analyze_sentiment(text):
    return sentiment_analyzer(text)[0]

def entity_summary_table(entities):
    summary = {}
    for e in entities:
        group = e['entity_group']
        summary[group] = summary.get(group, 0) + 1
    return summary

# ===========================
# Main Program
# ===========================
def main():
    print("=== Smart Document Analyzer (Colab Version - No Q&A) ===\n")

    print("Choose input method:")
    print("1. Upload a file (PDF/DOCX)")
    print("2. Paste text manually")
    choice = input("Enter 1 or 2: ").strip()

    text = ""

    # Option 1: File Upload
    if choice == "1":
        print("Upload your PDF or DOCX file:")
        uploaded = files.upload()
        if not uploaded:
            print("No file uploaded.")
            return
        filename = list(uploaded.keys())[0]
        print(f"Processing: {filename}")
        if filename.lower().endswith(".pdf"):
            text = extract_text_from_pdf(filename)
        elif filename.lower().endswith(".docx"):
            text = extract_text_from_docx(filename)
        else:
            print("Unsupported file type! Use PDF or DOCX.")
            return

    # Option 2: Manual Text Input
    elif choice == "2":
        print("Paste your text below. Press Enter twice to finish:")
        lines = []
        while True:
            line = input()
            if line == "":
                break
            lines.append(line)
        text = "\n".join(lines)

    else:
        print("Invalid choice!")
        return

    if not text.strip():
        print("No text provided.")
        return

    # --- Perform NLP Tasks ---
    print("\n--- Document Summary ---")
    print(summarize_text(text))

    print("\n--- Named Entities (Filtered & Merged) ---")
    entities = extract_entities(text)
    if entities:
        for entity in entities:
            print(f"{entity['word']} ({entity['entity_group']})")
    else:
        print("No high-confidence entities found.")

    print("\n--- Entity Summary Table ---")
    summary = entity_summary_table(entities)
    for group, count in summary.items():
        print(f"{group}: {count}")

    print("\n--- Top Keywords ---")
    keywords = extract_keywords(text)
    print(", ".join(keywords))

    print("\n--- Sentiment Analysis ---")
    sentiment = analyze_sentiment(text)
    print(f"Label: {sentiment['label']}, Score: {sentiment['score']:.2f}")

# ===========================
# Run Program
# ===========================
if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
!git config --global user.name "SreeharshaMansani"
!git config --global user.email "smansani07@gmail.com"
!git clone https://github.com/SreeharshaMansani/SmartDocumentAnalyzer.git
!cp SmartDocumentAnalyzer.py SmartDocumentAnalyzer/
# %cd SmartDocumentAnalyzer
!git add SmartDocumentAnalyzer.py
!git commit -m "Initial commit"
!git push origin main